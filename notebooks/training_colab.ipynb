{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ”¬ Semiconductor Defect Detection - Training Notebook\n",
                "\n",
                "This notebook trains EfficientNet-B0 and MobileNetV3-Small models for semiconductor defect classification.\n",
                "\n",
                "**Usage on Google Colab:**\n",
                "1. Upload your dataset to Google Drive\n",
                "2. Mount Drive and update DATA_DIR path\n",
                "3. Run all cells\n",
                "\n",
                "**Target:** 94-95% accuracy, <2MB model size"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies (Colab)\n",
                "!pip install albumentations imagehash tf2onnx -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive (for Colab)\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers, regularizers, Model\n",
                "from tensorflow.keras.applications import EfficientNetB0, MobileNetV3Small\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "from datetime import datetime\n",
                "\n",
                "print(f\"TensorFlow: {tf.__version__}\")\n",
                "print(f\"GPUs: {tf.config.list_physical_devices('GPU')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“ Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# UPDATE THIS PATH for your data location\n",
                "DATA_DIR = Path('/content/drive/MyDrive/semiconductor_data/augmented')\n",
                "# Or for local: DATA_DIR = Path('k:/iesa/data/augmented')\n",
                "\n",
                "OUTPUT_DIR = Path('/content/outputs')\n",
                "OUTPUT_DIR.mkdir(exist_ok=True)\n",
                "\n",
                "# Classes\n",
                "CLASS_NAMES = [\n",
                "    \"clean\", \"scratches\", \"particles\", \"pattern_defects\",\n",
                "    \"edge_defects\", \"center_defects\", \"random_defects\", \"other\"\n",
                "]\n",
                "NUM_CLASSES = 8\n",
                "IMAGE_SIZE = (224, 224)\n",
                "INPUT_SHAPE = (224, 224, 3)\n",
                "\n",
                "# Training\n",
                "BATCH_SIZE = 32\n",
                "STAGE1_EPOCHS = 15\n",
                "STAGE2_EPOCHS = 10\n",
                "STAGE1_LR = 1e-3\n",
                "STAGE2_LR = 1e-5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“Š Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_dataset(data_dir, split, batch_size=32, augment=False):\n",
                "    \"\"\"Create tf.data.Dataset from directory structure\"\"\"\n",
                "    split_dir = data_dir / split\n",
                "    \n",
                "    image_paths = []\n",
                "    labels = []\n",
                "    \n",
                "    for class_idx, class_name in enumerate(CLASS_NAMES):\n",
                "        class_dir = split_dir / class_name\n",
                "        if not class_dir.exists():\n",
                "            continue\n",
                "        for img_path in class_dir.glob(\"*.png\"):\n",
                "            image_paths.append(str(img_path))\n",
                "            labels.append(class_idx)\n",
                "    \n",
                "    def parse_image(path, label):\n",
                "        img = tf.io.read_file(path)\n",
                "        img = tf.image.decode_png(img, channels=1)\n",
                "        img = tf.image.grayscale_to_rgb(img)\n",
                "        img = tf.image.resize(img, IMAGE_SIZE)\n",
                "        img = tf.cast(img, tf.float32) / 255.0\n",
                "        return img, tf.one_hot(label, NUM_CLASSES)\n",
                "    \n",
                "    def augment_fn(image, label):\n",
                "        image = tf.image.random_flip_left_right(image)\n",
                "        image = tf.image.random_flip_up_down(image)\n",
                "        image = tf.image.random_brightness(image, 0.1)\n",
                "        image = tf.clip_by_value(image, 0.0, 1.0)\n",
                "        return image, label\n",
                "    \n",
                "    ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
                "    ds = ds.shuffle(len(image_paths))\n",
                "    ds = ds.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
                "    ds = ds.cache()\n",
                "    if augment:\n",
                "        ds = ds.map(augment_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
                "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
                "    return ds\n",
                "\n",
                "# Load datasets\n",
                "train_ds = create_dataset(DATA_DIR, 'train', BATCH_SIZE, augment=True)\n",
                "val_ds = create_dataset(DATA_DIR, 'val', BATCH_SIZE)\n",
                "test_ds = create_dataset(DATA_DIR, 'test', BATCH_SIZE)\n",
                "\n",
                "print(f\"Train: {len(list(train_ds))} batches\")\n",
                "print(f\"Val: {len(list(val_ds))} batches\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ—ï¸ Model Creation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_efficientnet(freeze_percent=0.8):\n",
                "    \"\"\"Create EfficientNet-B0 with custom head\"\"\"\n",
                "    inputs = layers.Input(shape=INPUT_SHAPE, name=\"input_image\")\n",
                "    \n",
                "    base = EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=INPUT_SHAPE)\n",
                "    \n",
                "    # Freeze layers\n",
                "    freeze_until = int(len(base.layers) * freeze_percent)\n",
                "    for i, layer in enumerate(base.layers):\n",
                "        layer.trainable = i >= freeze_until\n",
                "    \n",
                "    x = base(inputs)\n",
                "    x = layers.GlobalAveragePooling2D()(x)\n",
                "    x = layers.Dropout(0.3)(x)\n",
                "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Dropout(0.2)(x)\n",
                "    outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
                "    \n",
                "    return Model(inputs, outputs, name=\"efficientnet_classifier\")\n",
                "\n",
                "def create_mobilenet(freeze_percent=0.75):\n",
                "    \"\"\"Create MobileNetV3-Small with custom head\"\"\"\n",
                "    inputs = layers.Input(shape=INPUT_SHAPE, name=\"input_image\")\n",
                "    \n",
                "    base = MobileNetV3Small(include_top=False, weights=\"imagenet\", input_shape=INPUT_SHAPE)\n",
                "    \n",
                "    freeze_until = int(len(base.layers) * freeze_percent)\n",
                "    for i, layer in enumerate(base.layers):\n",
                "        layer.trainable = i >= freeze_until\n",
                "    \n",
                "    x = base(inputs)\n",
                "    x = layers.GlobalAveragePooling2D()(x)\n",
                "    x = layers.Dropout(0.3)(x)\n",
                "    x = layers.Dense(96, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Dropout(0.2)(x)\n",
                "    outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
                "    \n",
                "    return Model(inputs, outputs, name=\"mobilenet_classifier\")\n",
                "\n",
                "# Create models\n",
                "eff_model = create_efficientnet()\n",
                "eff_model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸš€ Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compile_model(model, lr):\n",
                "    model.compile(\n",
                "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
                "        loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
                "        metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
                "    )\n",
                "    return model\n",
                "\n",
                "def get_callbacks(name):\n",
                "    return [\n",
                "        keras.callbacks.ModelCheckpoint(\n",
                "            f'{OUTPUT_DIR}/{name}_best.keras',\n",
                "            monitor='val_accuracy', save_best_only=True\n",
                "        ),\n",
                "        keras.callbacks.EarlyStopping(\n",
                "            monitor='val_accuracy', patience=5, restore_best_weights=True\n",
                "        ),\n",
                "        keras.callbacks.ReduceLROnPlateau(\n",
                "            monitor='val_loss', factor=0.5, patience=3\n",
                "        )\n",
                "    ]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stage 1: Transfer Learning\n",
                "print(\"=\" * 50)\n",
                "print(\"STAGE 1: Transfer Learning\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "eff_model = compile_model(eff_model, STAGE1_LR)\n",
                "\n",
                "history1 = eff_model.fit(\n",
                "    train_ds,\n",
                "    validation_data=val_ds,\n",
                "    epochs=STAGE1_EPOCHS,\n",
                "    callbacks=get_callbacks('efficientnet_s1')\n",
                ")\n",
                "\n",
                "print(f\"\\nStage 1 Best Val Accuracy: {max(history1.history['val_accuracy']):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stage 2: Fine-tuning (unfreeze more layers)\n",
                "print(\"=\" * 50)\n",
                "print(\"STAGE 2: Fine-tuning\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Unfreeze more layers\n",
                "base = eff_model.layers[1]  # EfficientNet base\n",
                "for layer in base.layers[-50:]:\n",
                "    layer.trainable = True\n",
                "\n",
                "eff_model = compile_model(eff_model, STAGE2_LR)\n",
                "\n",
                "history2 = eff_model.fit(\n",
                "    train_ds,\n",
                "    validation_data=val_ds,\n",
                "    epochs=STAGE2_EPOCHS,\n",
                "    callbacks=get_callbacks('efficientnet_s2')\n",
                ")\n",
                "\n",
                "print(f\"\\nStage 2 Best Val Accuracy: {max(history2.history['val_accuracy']):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate on test set\n",
                "print(\"=\" * 50)\n",
                "print(\"EVALUATION\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "results = eff_model.evaluate(test_ds)\n",
                "print(f\"\\nTest Accuracy: {results[1]:.4f}\")\n",
                "print(f\"Test Precision: {results[2]:.4f}\")\n",
                "print(f\"Test Recall: {results[3]:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save models\n",
                "eff_model.save(f'{OUTPUT_DIR}/efficientnet_final.keras')\n",
                "eff_model.save(f'{OUTPUT_DIR}/efficientnet_final.h5')\n",
                "print(\"Models saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“¦ Export to ONNX"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export to ONNX (Phase 1 requirement)\n",
                "import tf2onnx\n",
                "\n",
                "spec = (tf.TensorSpec((None, 224, 224, 3), tf.float32, name=\"input\"),)\n",
                "output_path = f\"{OUTPUT_DIR}/efficientnet_defect_classifier.onnx\"\n",
                "\n",
                "model_proto, _ = tf2onnx.convert.from_keras(eff_model, input_signature=spec, output_path=output_path)\n",
                "print(f\"ONNX model saved to: {output_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quantize to TFLite (for edge deployment)\n",
                "converter = tf.lite.TFLiteConverter.from_keras_model(eff_model)\n",
                "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "converter.target_spec.supported_types = [tf.int8]\n",
                "\n",
                "tflite_model = converter.convert()\n",
                "\n",
                "tflite_path = f\"{OUTPUT_DIR}/efficientnet_quantized.tflite\"\n",
                "with open(tflite_path, 'wb') as f:\n",
                "    f.write(tflite_model)\n",
                "\n",
                "print(f\"TFLite model saved: {tflite_path}\")\n",
                "print(f\"Size: {len(tflite_model) / 1024 / 1024:.2f} MB\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}